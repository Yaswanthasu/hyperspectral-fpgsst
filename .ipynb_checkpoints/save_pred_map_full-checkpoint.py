{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adeecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "save_pred_map_full.py\n",
    "---------------------\n",
    "Create full-resolution predicted maps by sliding a patch across the whole HSI cube.\n",
    "\n",
    "Usage example:\n",
    "  python save_pred_map_full.py --dataset PaviaU --model FPGSST \\\n",
    "    --mat datasets/PaviaU/PaviaU.mat \\\n",
    "    --ckpt experiments/PaviaU/FPGSST/FPGSST_best.pth \\\n",
    "    --patch 9 --batch 512 --out results\n",
    "\n",
    "Notes:\n",
    " - This will run inference over every pixel (center of patch) and can take time.\n",
    " - Uses batching to avoid many tiny forward passes.\n",
    " - Requires your model class in models/<modelname>.py with class named <ModelName>.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import importlib\n",
    "from scipy.io import loadmat, savemat\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_hsi_from_mat(mat_path):\n",
    "    \"\"\"Load the first non-meta array from a .mat file and return HxWxB float32.\"\"\"\n",
    "    d = loadmat(mat_path)\n",
    "    # pick first non-__ key\n",
    "    keys = [k for k in d.keys() if not k.startswith(\"__\")]\n",
    "    if not keys:\n",
    "        raise RuntimeError(f\"No arrays found in {mat_path}\")\n",
    "    arr = d[keys[0]]\n",
    "    arr = np.squeeze(arr)\n",
    "    if arr.ndim != 3:\n",
    "        # maybe transposed (B,H,W) or (H,B,W) â€” try to find last dim as bands\n",
    "        if arr.shape[0] < arr.shape[-1]:\n",
    "            # attempt transpose if shape looks like (bands, H, W)\n",
    "            arr = np.transpose(arr, (1, 2, 0))\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def sliding_patch_inference(model, X, patch_size=9, batch_size=512, device=\"cuda\"):\n",
    "    \"\"\"Run sliding-window inference over X (H,W,B). Returns pred_map (H,W).\"\"\"\n",
    "    H, W, B = X.shape\n",
    "    pad = patch_size // 2\n",
    "    padded = np.pad(X, ((pad, pad), (pad, pad), (0, 0)), mode=\"reflect\")\n",
    "\n",
    "    model.eval()\n",
    "    pred_map = np.zeros((H, W), dtype=np.int32)\n",
    "\n",
    "    coords = []\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            coords.append((i, j))\n",
    "\n",
    "    # Prepare batches of patches\n",
    "    with torch.no_grad():\n",
    "        for start in tqdm(range(0, len(coords), batch_size), desc=\"Batches\"):\n",
    "            batch_coords = coords[start:start+batch_size]\n",
    "            batch_patches = np.empty((len(batch_coords), B, patch_size, patch_size), dtype=np.float32)\n",
    "            for idx, (i, j) in enumerate(batch_coords):\n",
    "                patch = padded[i:i+patch_size, j:j+patch_size, :]  # (patch,patch,B)\n",
    "                patch = np.transpose(patch, (2, 0, 1))  # -> (B, patch, patch)\n",
    "                batch_patches[idx] = patch\n",
    "            batch_tensor = torch.from_numpy(batch_patches).to(device)\n",
    "            outputs = model(batch_tensor)  # assume outputs shape (N, num_classes)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            for k, (i, j) in enumerate(batch_coords):\n",
    "                pred_map[i, j] = int(preds[k])\n",
    "    return pred_map\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Create full predicted maps via sliding-window inference\")\n",
    "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"Dataset name (used for default paths)\")\n",
    "    parser.add_argument(\"--mat\", type=str, default=None, help=\"Path to original HSI .mat (H x W x B). If omitted, uses datasets/<dataset>/<dataset>.mat\")\n",
    "    parser.add_argument(\"--ckpt\", type=str, required=True, help=\"Checkpoint .pth (must contain 'model_state')\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"FPGSST\", help=\"Model class name (and module name lowercase in models/)\")\n",
    "    parser.add_argument(\"--patch\", type=int, default=9, help=\"Patch size (odd)\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=512, help=\"Batch size for inference\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"Device to run on (cuda or cpu)\")\n",
    "    parser.add_argument(\"--out\", type=str, default=\"results\", help=\"Output folder for pred_map .mat\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Decide MAT path\n",
    "    if args.mat:\n",
    "        mat_path = args.mat\n",
    "    else:\n",
    "        mat_path = os.path.join(\"datasets\", args.dataset, f\"{args.dataset}.mat\")\n",
    "    assert os.path.exists(mat_path), f\"MAT file not found: {mat_path}\"\n",
    "    assert os.path.exists(args.ckpt), f\"Checkpoint not found: {args.ckpt}\"\n",
    "\n",
    "    print(\"Loading HSI from:\", mat_path)\n",
    "    X = load_hsi_from_mat(mat_path)  # H,W,B\n",
    "    H, W, B = X.shape\n",
    "    print(f\"HSI shape: {H} x {W} x {B}\")\n",
    "\n",
    "    # Load model\n",
    "    model_module = importlib.import_module(f\"models.{args.model.lower()}\")\n",
    "    model_class = getattr(model_module, args.model)\n",
    "    model = model_class(in_bands=B, num_classes= int(10), patch_size=args.patch) if \"patch_size\" in model_class.__init__.__code__.co_varnames else model_class(in_bands=B, num_classes= int(10))\n",
    "    # The above tries to instantiate; we'll replace num_classes properly after loading ckpt\n",
    "\n",
    "    # Load checkpoint to detect num_classes and load weights\n",
    "    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n",
    "    if \"model_state\" in ckpt:\n",
    "        state = ckpt[\"model_state\"]\n",
    "    else:\n",
    "        state = ckpt\n",
    "    # infer num_classes from state if possible: skip, instead we reload with a temporary approach:\n",
    "    # load temporary model on cpu for shape inspection if mismatch; easier: get num_classes from model final layer size if present\n",
    "    # Instead, we try to inspect the final linear layer weight size in state dict\n",
    "    final_out = None\n",
    "    for k in state.keys():\n",
    "        if k.endswith(\".weight\"):\n",
    "            if len(state[k].shape) == 2:\n",
    "                # track largest second-dim? skip\n",
    "                final_out = state[k].shape[0]\n",
    "    if final_out is None:\n",
    "        raise RuntimeError(\"Could not determine num_classes from checkpoint. Ensure it saves 'model_state' dict.\")\n",
    "    num_classes = int(final_out)\n",
    "\n",
    "    # Recreate model with correct num_classes\n",
    "    # prefer constructor with patch_size parameter if available\n",
    "    model_args = {}\n",
    "    ctor = model_class.__init__.__code__.co_varnames\n",
    "    if \"patch_size\" in ctor:\n",
    "        model = model_class(in_bands=B, num_classes=num_classes, patch_size=args.patch)\n",
    "    else:\n",
    "        model = model_class(in_bands=B, num_classes=num_classes)\n",
    "    device = torch.device(args.device if torch.cuda.is_available() and \"cuda\" in args.device else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # load state\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded: {args.model} with {num_classes} classes on {device}\")\n",
    "\n",
    "    # Run sliding-window inference in batches\n",
    "    pred_map = sliding_patch_inference(model, X, patch_size=args.patch, batch_size=args.batch, device=device)\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "    out_path = os.path.join(args.out, f\"pred_map_{args.dataset}.mat\")\n",
    "    savemat(out_path, {\"pred_map\": pred_map})\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
